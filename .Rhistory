data.train[, N:=.N, by =list(linkId.from, linkId.to, timeBins) ]
# A.1 Variance and mean velocity computation
# mu and sd2: For each ijk link, we calculate the mean and the variance of the speed for all
# the times the ijk link is taken. Consequently, we also obtain NA values
# for the variance of the speed of the ijk links which were taken only 1 time (N = 1) in the data set.
graph.stat = data.train[, .(mu = mean(1/speed),  sd2 = var(1/speed), N=N[1]) ,by = list(linkId.from, linkId.to, timeBins)]
# We create the structure of the graph network
full.graph = graph.stat[, .(timeBins = data.timebins) ,by = list(linkId.from, linkId.to)]
# We merge mu and sd2 calculated above to the structure of our graph network
graph.stat = merge(full.graph, graph.stat, by= c('linkId.from', 'linkId.to', 'timeBins'), all.x = TRUE)
graph.stat$N[is.na(graph.stat$N)]<-0
# mu.from.tb and sd2.from.tb: We calculate the mean and the variance of the speed
# for all the times when the ik link (linkid.from and timebins) is borrowed more
# than 10 times (without looking at the destination).
# That is to say that the value of mu.from.tb and sd2.from.tb exists if and only if,
# the link ik has a sum of N greater than 10 (where 10 is a hyperparameter )
graph.stat.from.tb = data.train[, .(mu = mean(1/speed),
sd2 = var(1/speed), N= .N) ,
by = list(linkId.from, timeBins)]
# mu.tb and sd2.tb: We calculate the mean and the variance of the speed for each timebins.
graph.stat.tb = data.train[, .(mu = mean(1/speed),
sd2 = var(1/speed), N= .N) ,by = list(timeBins)]
# we combine the three in the same data frame
graph.stat.full =  merge(graph.stat,
graph.stat.from.tb[N>L, .(mu.from.tb = mu,
sd2.from.tb = sd2, linkId.from, timeBins)],
all.x = TRUE,
by.x=c('linkId.from', 'timeBins'),
by.y = c('linkId.from', 'timeBins'))
graph.stat.full =  merge(graph.stat.full,
graph.stat.tb[N>L, .(mu.tb = mu, sd2.tb = sd2, timeBins)],
all.x = TRUE, by.x=c('timeBins'), by.y = c('timeBins'))
# A.2 For each ijk link, if the value of mu and / or sd2 is missing, then the variance and the mean
# of the ijk link are the values of mu.from.tb and sd2.from.tb.
# If these values are also missing, then the mean and the variance of the ijk link become
# the values of mu.tb and sd2.tb.
# ( Sd2 is actually the standard deviation so we calculate the square root of sd2 to find the variance.)
graph.stat.full[, muspeed := ifelse(is.na(mu) | N < L, ifelse(is.na(mu.from.tb), mu.tb, mu.from.tb), mu)]
graph.stat.full[,  imputed_mean := ifelse(muspeed== mu, FALSE, TRUE )]
graph.stat.full[, sdspeed := ifelse(is.na(sd2), ifelse(is.na(sd2.from.tb), sd2.tb, sd2.from.tb), sd2)]
graph.stat.full[, imputed_sd := ifelse(sdspeed == sd2  & !is.na(sd2) , FALSE, TRUE)]
graph.stat.full = graph.stat.full[,.(linkId.from,
linkId.to,
timeBins,
mean = muspeed,
sd = sqrt(sdspeed),
imputed_sd,
imputed_mean )]
# Merging with the train dataset
data.train = merge(data.train, graph.stat.full, all.x = TRUE, by = c('linkId.from', 'linkId.to', 'timeBins'), sort = FALSE)[order(trip,time)]
# Returning variables
graph <- list(graph.stat.full = graph.stat.full, data.train = data.train)
class(graph) = append(class(graph), "graph", after=0)
graph
}
sample_data <- function(data.train = NULL, bin = NULL, M = NULL){
# A.3 Sampling train dataset according to model input
t = data.train[, .N , by=trip][N>10][, trip]
sample_timebin = data.train[trip %in% t, .(timeBins= if(all(timeBins == timeBins[1])) timeBins[1] else 'remove' ), by=trip]
sample_timebin = sample_timebin[!timeBins == 'remove']
if(is.null(bin)){
samp = sample_timebin[, trip]
tt = data.train
xx = tt[, .I[.N>10],by = trip]
} else {
samp = sample_timebin[timeBins == bin][, trip]
samp = samp[sample.int(length(samp),M)]
tt = data.train[trip %in% samp]
xx = tt[trip %in% samp, .I[.N>10],by = trip]
}
samp_data <- list(samp = samp, tt = tt, xx = xx)
class(samp_data) <- append(class(samp_data), "samp_data", after = 0)
samp_data
}
get_rho <- function(tt = NULL, xx = NULL){
# A.4 Get rho. The algorithm only keeps trips ("trips") that have more than 10 ijk links.
# On these links, we will create and use a function called "get rho" allowing to calculate
# the autocovariance of the speed with a lag up to 5 for each trip.
# If desired, the function also allows calculation specifically for AM or PM timebins.
rho = tt[xx$V1][,    drop((acf((1/speed - mean)/(sd+1e-5), plot=FALSE, lag.max=5))[[1]]), by = trip]
a = rho[, V1[2] ,by = trip][, V1]
rho = round(mean(a), 2)
print(paste0("Mean of autocorrelation: ", rho))
getrho <- list(rho = rho, tt = tt, xx = xx)
class(getrho) <- append(class(getrho), "rho", after = 0)
getrho
}
param_zeta<-function(t0, rho,linkfrom, linkto, len, sequence = FALSE, rules = NULL, graph.stat.full = NULL){
t0 = as.POSIXlt(t0)
time_bins <- rules2timebins(rules)
tbin = time_bins(t0)
g = graph.stat.full[linkId.from %in% linkfrom]
t = 0
vcumlative = 0
vprev = 1
lenprev = 1
if(sequence){
t.seq = v.seq = numeric(length(len))
}
for(i  in 1:(length(len) - 1)){
d = g[linkId.from == linkfrom[i] & linkId.to == linkto[i] & timeBins == tbin]
t = t + len[i]*d$mean
tbin = time_bins(t0 + t)
vcumlative = vcumlative + len[i]^2  * d$sd^2  + 2*rho* len[i]*lenprev * d$sd * vprev
vprev = d$sd
lenprev = len[i]
if(sequence){
v.seq[i] = vcumlative
t.seq[i] = t
}
}
i = i+1
if(is.na(linkto[i])){
d = g[linkId.from == linkfrom[i] & timeBins == tbin & is.na(linkId.to)]
}else{
d = g[linkId.from == linkfrom[i] & linkId.to == linkto[i] & timeBins == tbin]
}
t = t + len[i]*d$mean
vcumlative = vcumlative + len[i]^2 * d$sd^2 + 2* rho * len[i]*lenprev *d$sd *vprev
if(sequence){
v.seq[i] = vcumlative
t.seq[i] = t
}
s = sqrt(vcumlative)
if(sequence){
list(tt =t, sd = s, v.seq = sqrt(v.seq), t.seq = t.seq)
}else{
list(tt =t, sd = s)
}}
traveltimeCLT <- function(data.train = NULL, M = NULL, L = NULL, bin = NULL, rules = NULL, data.timebins = NULL){
graph_obj <- create_graph(data.train, L, data.timebins)
samp_obj <- sample_data(graph_obj$data.train, bin = bin, M = M)
rho_obj <- get_rho(tt = samp_obj$tt, xx= samp_obj$xx)
# A.6 Residual variance.
# We create a residual variance function, allowing to supply the residual variance taking as input: DB (data), rho, etsamp = NULL.
# This function uses the param_zeta function seen above.
# First, it calculates obstt, which is the sum of tt (travel time) for each trip.
# Then we add to the data D a column of the residues called res and calculated in the following way:
if(is.null(samp_obj$samp)){
A = graph_obj$data.train[order(trip, time)]
} else{
A = graph_obj$data.train[trip %in% samp_obj$samp][order(trip, time)]}
B = A[, param_zeta(time[1], rho_obj$rho, linkId.from, linkId.to, length, rules = rules, graph.stat.full = graph_obj$graph.stat.full), by = trip]
D = merge(B, A[, .(obstt= sum(tt)), trip])
D[, res := (obstt - tt)/sd]
res = list(db = D, res.sd = sd(D$res))
v = res$res.sd
print("This is the mean of the residuals"); print(v)
#Returning variables
traveltimeCLT_obj <- list(variance = v, rho = rho_obj$rho, graph.stat.full = graph_obj$graph.stat.full)
class(traveltimeCLT_obj) = append(class(traveltimeCLT_obj), "traveltimeCLT_obj", after=0)
invisible(traveltimeCLT_obj)
}
predict_traveltimeCLT <- function(traveltimeCLT_obj = NULL, data.test = NULL, bin = NULL , rules = NULL){
# A.0 We are starting to transform our data so that it takes on a network form.
# So, we transform the linkId variable into two variables: LinkId.from and LinkId.to.
# Since a link represents a segment of a road network,
# we can thus know the origin and the destination of a vehicle for each link of this network.
# We add a variable called N, which represents the number of times that an ijk link
# (i = linkId.from, j = linkId.to etk = timebins) is present in our data set.
# Since there is no destination j (linkId.to) for the last ijk link of the trip of a vehicle,
# we consider its value as NA.
data.test[, linkId.from := linkId, by = trip]
data.test[, linkId.to := shift(linkId, type = 'lead'), by = trip]
data.test[, N:=.N, by =list(linkId.from, linkId.to, timeBins) ]
# Sampling test data according to the model inputs
samp.test = data.test[, timeBins[1], trip][V1 == bin , trip]
# A.2 Residual variance.
# We create a residual variance function, allowing to supply the residual variance taking as input: DB (data), rho, etsamp = NULL.
# This function uses the param_zeta function seen above.
# First, it calculates obstt, which is the sum of tt (travel time) for each trip.
# Then we add to the data D a column of the residues called res and calculated in the following way:
A = data.test[trip %in% samp.test, param_zeta(time[1], traveltimeCLT_obj$rho, linkId.from, linkId.to, length, rules = rules, graph.stat.full = traveltimeCLT_obj$graph.stat.full), by = trip]
# We merge to be able to get the observed travel time, the real travel time, and the variance
B = merge(data.test[, .(obstt = sum(tt)), trip], A)
# We get the results using the mean of the residuals v
print("Model estimation using mean of the residuals v")
print(t(B[ , numerical_res(obstt, tt, sd *qnorm(0.975)*traveltimeCLT_obj$variance)]))
# We get the results without using the mean of the residuals v
print("Model estimation without using mean of the residuals v")
print(t(B[ , numerical_res(obstt, tt, sd *qnorm(0.975))]))
}
# A.3 We get the model estimations
numerical_res<-function(tt, est, sym.q){
list(
MAREgeo = 100*exp(mean(log(abs(tt - est)/tt))),
RMSE = sqrt(mean((tt - est)^2)),
MAE  = mean(abs(tt - est)),
ME  = mean(tt- est),
MAPE = 100*mean(abs(tt - est)/tt),
emprical.cov = 100*mean(abs(tt-est)<=sym.q),
PI.length = mean(2*sym.q),
PI.rel.length = 100*mean(2*sym.q/tt)
)
}
create_test_trips<-function(M = 500, db, min.n = 1){
db[, linkId.from := linkId, by = trip]
db[, linkId.to := shift(linkId, type = 'lead'), by = trip]
db[, N:=.N, by =list(linkId.from, linkId.to, timeBins) ]
A = db[, .(N = N[1]), by =list(linkId.from, linkId.to, timeBins) ]
A[, N.left := 0 ]
setkey(A, linkId.from, linkId.to, timeBins)
bank = c()
unique.trips =db[, min(N>1)  , trip][V1 ==1][, trip]
repeat{
n = length(unique.trips)
car = unique.trips[sample.int(n,1)]
k = db[trip==car, .(linkId.from, linkId.to, timeBins)]
if(A[k, all(N -N.left - 1> 0 )]){
bank = c(bank,car)              ## add the trip
unique.trips = setdiff(unique.trips, bank) ## removing all smapled trips
## updating A
A[k, N.left:= N.left + 1]
## break if number of samples reached
if(length(bank) >=M)
break
}
}
bank
}
# Loading data
Sumo_data <- read.csv("Quebec_data/Real_data.csv")
Sumo_data$speed <- exp(Sumo_data$logspeed)
# Transforming variables
Sumo_data <- as.data.table(Sumo_data)
Sumo_data$timeBins <- as.character(Sumo_data$timeBins)
# Creating test trips
set.seed(2020)
test.trips.Sumo <- create_test_trips(M = 500, Sumo_data, min.n = 1)
# Splitting data into train and test set
test = Sumo_data[trip %in% test.trips.Sumo]
train = Sumo_data[!trip %in% test.trips.Sumo]
# Counting the the number of trips in each set
print(paste0("Number of trips inside the test set: ", test[, 1, trip][, sum(V1)]))
print(paste0("Number of trips inside the train set: ", train[, 1, trip][, sum(V1)]))
print(paste0("Number of trips in total: ", test[, 1, trip][, sum(V1)] + train[, 1, trip][, sum(V1)]))
# Setting up the rules for our dataset
myrules = list(
list(start='6:30', end= '9:00', days = 0:6, tag='MR'),
list(start='15:00', end= '18:00', days = 0:6, tag='ER')
)
mytimebins = c("MR", "ER", "Other")
ttCLTmodel <- traveltimeCLT(data.train = train,
M = 1000,
L = 2,
bin = "MR",
rules = myrules,
data.timebins = mytimebins)
ttCLTresults <- predict_traveltimeCLT(ttCLTmodel, test, "MR", myrules)
library(traveltimeCLT)
library(data.table)
install.packages('data.table')
library(traveltimeCLT)
library(dplyr)
library(tidyr)
?traveltimeCLT
library(traveltimeHMM)
library(traveltimeCLT)
library(dplyr)
library(tidyr)
library(data.table)
# Loading data
Sumo_data <- read.csv("Quebec_data/Real_data.csv")
Sumo_data$speed <- exp(Sumo_data$logspeed)
# Transforming variables
Sumo_data <- as.data.table(Sumo_data)
Sumo_data$timeBins <- as.character(Sumo_data$timeBins)
# Creating test trips
set.seed(2020)
test.trips.Sumo <- create_test_trips(M = 500, Sumo_data, min.n = 1)
# Splitting data into train and test set
test = Sumo_data[trip %in% test.trips.Sumo]
train = Sumo_data[!trip %in% test.trips.Sumo]
# Counting the the number of trips in each set
print(paste0("Number of trips inside the test set: ", test[, 1, trip][, sum(V1)]))
print(paste0("Number of trips inside the train set: ", train[, 1, trip][, sum(V1)]))
print(paste0("Number of trips in total: ", test[, 1, trip][, sum(V1)] + train[, 1, trip][, sum(V1)]))
# Setting up the rules for our dataset
myrules = list(
list(start='6:30', end= '9:00', days = 0:6, tag='MR'),
list(start='15:00', end= '18:00', days = 0:6, tag='ER')
)
mytimebins = c("MR", "ER", "Other")
ttCLTmodel <- traveltimeCLT(data.train = train,
M = 1000,
L = 2,
bin = "MR",
rules = myrules,
data.timebins = mytimebins)
ttCLTresults <- predict_traveltimeCLT(ttCLTmodel,
test,
"MR",
myrules)
# Loading data
Sumo_data <- read.csv("Quebec_data/Sumo_data.csv")
Sumo_data$speed <- exp(Sumo_data$logspeed)
# Transforming variables
Sumo_data <- as.data.table(Sumo_data)
Sumo_data$timeBins <- as.character(Sumo_data$timeBins)
# Creating test trips
set.seed(2020)
test.trips.Sumo <- create_test_trips(M = 500, Sumo_data, min.n = 1)
# Splitting data into train and test set
test = Sumo_data[trip %in% test.trips.Sumo]
train = Sumo_data[!trip %in% test.trips.Sumo]
# Counting the the number of trips in each set
print(paste0("Number of trips inside the test set: ", test[, 1, trip][, sum(V1)]))
print(paste0("Number of trips inside the train set: ", train[, 1, trip][, sum(V1)]))
print(paste0("Number of trips in total: ", test[, 1, trip][, sum(V1)] + train[, 1, trip][, sum(V1)]))
# Setting up the rules for our dataset
myrules = list(
list(start='6:30', end= '9:00', days = 0:6, tag='MR'),
list(start='15:00', end= '18:00', days = 0:6, tag='ER')
)
mytimebins = c("MR", "ER", "Other")
# Setting up the rules for our dataset
myrules = list(
#list(start='6:30', end= '9:00', days = 0:6, tag='MR'),
#list(start='15:00', end= '18:00', days = 0:6, tag='ER')
list(start='7:00', end= '10:00', days = 0:6, tag='MR'),
list(start='12:00', end='15:00', days = 0:6, tag='NR'),
list(start='17:00', end= '20:00', days = 0:6, tag='ER')
)
mytimebins = c("MR", "NR", "ER", "Other")
ttCLTmodel <- traveltimeCLT(data.train = train,
M = 1000,
L = 2,
bin = "MR",
rules = myrules,
data.timebins = mytimebins)
ttCLTresults <- predict_traveltimeCLT(ttCLTmodel,
test,
"MR",
myrules)
# Loading libraries
library(dplyr)
library(tidyr)
library(data.table)
# Reading data
mydata <- read.csv("Luxembourg_data/vehroutedata.csv")
# Turning data into a correct format
Correct_Shape <- function(data_input){
data_input$real_id <- data_input$id
data_input$id <- sample(1:300000, nrow(data_input), replace=FALSE)
data_input$edge <- as.character(data_input$edge)
data_input$exitTimes <- as.character(data_input$exitTimes)
s <- strsplit(data_input$edge, split = " ")
s1 <- data.frame(id = rep(data_input$id, sapply(s, length)), edge = unlist(s))
s <- strsplit(data_input$exitTimes, split = " ")
s2 <- data.frame(id = rep(data_input$id, sapply(s, length)), exitTimes = unlist(s))
Sumo_s1 <- s1 %>% group_by(id)%>%tally()
Sumo_s2 <- s2 %>% group_by(id)%>%tally()
clean <- setdiff(Sumo_s1, Sumo_s2)
data_input <- data_input[!data_input$id %in% clean$id,]
data_edge <- data_input %>%
separate_rows(edge, sep=" ")
data_exitTimes <- data_input %>%
separate_rows(exitTimes, sep=" ")
data_edge <- data_edge[data_edge$edge!="",]
data_edge$exitTimes <- data_exitTimes$exitTimes
data <- data_edge
data$depart <- as.integer(data$depart)
data$exitTimes <- as.integer(data$exitTimes)
data$tt <- data$exitTimes - data$depart
data$time <- "2020-07-08 00:00:00"
data$time <- as.POSIXct(data$time)
return(data)
}
data <- Correct_Shape(mydata)
# Setting the graph
Setting_graph <- function(data_input){
data_input <- as.data.table(data_input)
data_input[, exitTimes2 := exitTimes, by = id]
data_input[, exitTimes2 := shift(exitTimes2, type = 'lag'), by = id]
data_input <- as.data.frame(data_input)
data_input$depart <- ifelse(!is.na(data_input$exitTimes2), data_input$exitTimes2, data_input$depart)
data_input$tt <- ifelse(!is.na(data_input$exitTimes2), (data_input$exitTimes-data_input$depart), data_input$tt)
data_input$time <- data_input$time + data_input$depart
data_input$type <- ifelse(!is.na(data_input$type), data_input[data_input$type!="bus",], data_input$type)
data_input$depart <- NULL
data_input$exitTimes2 <- NULL
data_input$exitTimes <- NULL
data_input$X <- NULL
return(data_input)
}
vehroutedata_cleaned <- Setting_graph(data)
write.csv(vehroutedata_cleaned, "Luxembourg_data/vehroutedata_cleaned.csv")
write.csv(vehroutedata_cleaned, "Luxembourg_data/vehroutedata_cleaned.csv")
vehroutedata_cleaned
library(dplyr)
library(tidyr)
library(data.table)
#install.packages("devtools")
#devtools::install_github("melmasri/traveltimeHMM")
library(traveltimeHMM)
library(ggplot2)
edges <- read.csv("Luxembourg_data/edges.csv")
vehrouteData <- read.csv("Luxembourg_data/vehroutedata_clean.csv")
## Format edges and keep edge ID and edge length
edges_file <- function(edges){
edges %>%
mutate(
edge = gsub(":", "", edge_id),
) %>%
select(edge, length)%>%
arrange(edge, length) %>%
group_by(edge)%>%
summarise(length = first(length))
}
edges <- edges_file(edges)
## Join edges to trip data to obtain edge length and rename columns
join_edges2trip <-function(data){
data$edge <- gsub("\t", "", data$edge)
vehrouteData %>%
left_join(edges, by ="edge") %>%
arrange(id, time) %>%
mutate(
traveltime = ifelse(tt==0, 0.5, tt),
tripID = id,
linkID = gsub("\t", "", edge),
logspeed = log(length/traveltime),
timeBin = 0,
speedkmh = (length/traveltime)*3.6,
time = gsub("\t", "", time)
) %>%
select(tripID, linkID, timeBin, logspeed, speedkmh, traveltime, length, time)
}
Sumo_data <- join_edges2trip(vehrouteData)
## Add time bins
rules = list(
list(start='7:00', end= '10:00', days = 0:6, tag='MR'),
list(start='12:00', end='15:00', days = 0:6, tag='NR'),
list(start='17:00', end= '20:00', days = 0:6, tag='ER')
#list(start='6:30', end= '9:00', days = 0:6, tag='MR'),
#list(start='15:00', end= '18:00', days = 0:6, tag='ER')
)
time_bins <- rules2timebins(rules)
Sumo_data$timeBin = time_bins(Sumo_data$time)
# Creating column speed
Sumo_data$speed <- exp(Sumo_data$logspeed)
# We remove trips whose one of the link speed is > 150
Sumo_data <- as.data.table(Sumo_data)
Sumo_data[, whour :=as.POSIXlt(time)$wday*24 + as.POSIXlt(time)$hour]
Sumo_data[, week :=week(time)]
r = Sumo_data[3.6*exp(logspeed)>150][, tripID[1],tripID][, tripID]
Sumo_data= Sumo_data[!tripID %in% r][order(tripID, time)]
# We remove the last link from each trip because the cars never end up their trip at the same time
#Sumo_data = Sumo_data[,.SD[-.N] , tripID]
#Sumo_data = Sumo_data[,.SD[-1] , tripID]
Sumo_data = Sumo_data[order(tripID, time)]
# We rename the columns to get the correct names for our method
Sumo_data <- as.data.frame(Sumo_data)
Sumo_data <- rename(Sumo_data, c("trip"="tripID", "timeBins"="timeBin", "linkId"="linkID", "tt"="traveltime"))
write.csv(Sumo_data, "Luxembourg_data/Sumo_data.csv")
library(ggplot2)
library(splitstackshape)
Sumo_data <- read.csv("Luxembourg_data/Sumo_data.csv")
Sumo_data_time <- data.frame(Data=as.POSIXct(Sumo_data$time, format="%Y-%m-%d %H:%M:%S"))
ggplot(Sumo_data_time, aes(x=Data)) +
geom_histogram(binwidth=100, fill="grey", colour="black") + #binwidth in seconds
ylab("Number of trips") + #xlab should pick up variable name 'Data'
ggtitle("Traffic on 24h")
# Visualization of the variation of the speed for a sample of trip
Sumo_data <- getanID(data = Sumo_data, id.vars = "trip")
set.seed(2)
sample_plot <- unique(Sumo_data$trip[Sumo_data$timeBins=="MR"])
sample_plot <- sample(sample_plot, 5)
Sumo_data_sample_plot <- Sumo_data[Sumo_data$trip %in% sample_plot]
Sumo_data_sample_plot$trip <- as.factor(Sumo_data_sample_plot$trip)
Sumo_data_sample_plot <- rename(Sumo_data_sample_plot, c("nbr_edge" = ".id"))
ggplot(Sumo_data_sample_plot, aes(x = Sumo_data_sample_plot$nbr_edge, y = Sumo_data_sample_plot$speedkmh, group = Sumo_data_sample_plot$trip, color = Sumo_data_sample_plot$trip)) + geom_line() + geom_point() + scale_colour_discrete(drop=TRUE,
limits = levels(Sumo_data_sample_plot$trip))
library(traveltimeHMM)
library(traveltimeCLT)
library(dplyr)
library(tidyr)
library(data.table)
# Loading data
Sumo_data <- read.csv("Luxembourg_data/Sumo_data.csv")
Sumo_data$speed <- exp(Sumo_data$logspeed)
# Transforming variables
Sumo_data <- as.data.table(Sumo_data)
Sumo_data$timeBins <- as.character(Sumo_data$timeBins)
# Creating test trips
set.seed(2020)
test.trips.Sumo <- create_test_trips(M = 500, Sumo_data, min.n = 1)
# Splitting data into train and test set
test = Sumo_data[trip %in% test.trips.Sumo]
train = Sumo_data[!trip %in% test.trips.Sumo]
# Counting the the number of trips in each set
print(paste0("Number of trips inside the test set: ", test[, 1, trip][, sum(V1)]))
print(paste0("Number of trips inside the train set: ", train[, 1, trip][, sum(V1)]))
print(paste0("Number of trips in total: ", test[, 1, trip][, sum(V1)] + train[, 1, trip][, sum(V1)]))
# Setting up the rules for our dataset
myrules = list(
#list(start='6:30', end= '9:00', days = 0:6, tag='MR'),
#list(start='15:00', end= '18:00', days = 0:6, tag='ER')
list(start='7:00', end= '10:00', days = 0:6, tag='MR'),
list(start='12:00', end='15:00', days = 0:6, tag='NR'),
list(start='17:00', end= '20:00', days = 0:6, tag='ER')
)
mytimebins = c("MR", "ER", "NR", "Other")
ttCLTmodel <- traveltimeCLT(data.train = train,
M = 1000,
L = 2,
bin = "MR",
rules = myrules,
data.timebins = mytimebins)
ttCLTresults <- predict_traveltimeCLT(ttCLTmodel,
test,
"MR",
myrules)
